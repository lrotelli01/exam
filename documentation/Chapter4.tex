\chapter{Data Analysis and Conclusions}

This chapter presents a statistical analysis of the simulation results, including warm-up period determination, factorial design analysis, normality testing, and residual analysis to validate the model assumptions.

\section{Warm-Up Period Analysis}

Before collecting steady-state statistics, it is essential to identify and discard the \textbf{transient phase} (warm-up period) during which the system has not yet reached equilibrium.

In this project, the warm-up period was estimated by \textbf{visual inspection} of the simulation trend.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/warmup_analysis.png}
\caption{Warm-up behavior observed in the simulation output.}
\label{fig:warmup_analysis}
\end{figure}

From Figure~\ref{fig:warmup_analysis}, it is clear that after the initial transient the curve stabilizes, so a \textbf{500 s warm-up} is considered sufficient.

\begin{tcolorbox}[title=Warm-Up Configuration]
The OMNeT++ configuration includes:
\begin{verbatim}
warmup-period = 500s
\end{verbatim}
\end{tcolorbox}

\section{Experimental Design}

The simulation study follows a \textbf{factorial design} with the following factors:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Factor} & \textbf{Symbol} & \textbf{Levels} \\
\hline
Number of users & $N$ & 100, 500, 1000, 1200, 1500, 1600, 2000, 2500, 3000, 3500, 4000, 5000 \\
Read probability & $p$ & 0.3, 0.5, 0.8 \\
Number of tables & $M$ & 20 \\
Distribution type & - & Uniform, Lognormal \\
\hline
\end{tabular}
\caption{Experimental factors and their levels}
\label{tab:factors}
\end{table}

Each configuration was replicated 5 times with different random seeds, resulting in a total of \textbf{360 simulation runs}.

\section{Factor Impact Analysis}

The pie chart [Figure~\ref{fig:factor_pie}] shows how much each factor contributes to \textbf{waiting-time} variability.
The largest contribution is still \textbf{Number of Users ($N$)}, but \textbf{Distribution} and interaction terms are also relevant.
This is coherent with the capacity results: hotspot traffic has a strong impact on delay growth and stall threshold.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/factor_effects_pie.pdf}
\caption{The pie chart illustrates the impact of each factor on waiting-time variability.}
\label{fig:factor_pie}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Effect term} & \textbf{Contribution (\%)} \\
\hline
$N$ & 33.51 \\
$p$ & 9.39 \\
Distribution & 22.56 \\
$N\times p$ & 7.16 \\
$N\times \mathrm{Dist}$ & 14.46 \\
$p\times \mathrm{Dist}$ & 3.73 \\
Residual & 9.19 \\
\hline
\end{tabular}
\caption{ANOVA-style contribution on $\log_{10}(W+1)$, where $W$ is waiting time in ms.}
\label{tab:factor_contrib_wait}
\end{table}

\noindent Important interpretation: the \textbf{main effect} of read probability is $9.39\%$, but the overall impact of read probability is larger if interactions are included:
\[
p + (N\times p) + (p\times \mathrm{Dist}) \approx 9.39 + 7.16 + 3.73 = 20.28\%.
\]
So read probability does not act ``in isolation''; it becomes much more influential when load and hotspot distribution change.

\subsection{Key Findings}

The analysis reveals that:

\begin{itemize}
    \item \textbf{Number of Users ($N$)}: This is the single largest driver (33.51\%). Increasing users pushes the system toward saturation and raises queueing delay.
    
    \item \textbf{Read Probability ($p$)}: The direct term is 9.39\%, but the effective impact reaches about 20.28\% when interactions are considered. This explains why changing $p$ can strongly improve delay in stressed scenarios.
    
    \item \textbf{Distribution}: A strong standalone contribution (22.56\%). Uniform access keeps delay controlled, while Lognormal hotspots amplify contention.

    \item \textbf{Interactions}: $N\times \mathrm{Dist}$ (14.46\%) and $N\times p$ (7.16\%) are substantial, meaning that the effect of one factor depends on the operating point of the others.

    \item \textbf{Residual}: 9.19\% indicates remaining variability not explained by first-order terms only (e.g., stochastic effects near threshold conditions), which is expected in saturation regions.
\end{itemize}


\section{Model Validation}

\subsection{Testing Normality Hypothesis}

For each $(N,p,\text{distribution})$ configuration, the reference waiting time is the mean over the 5 runs, and residuals are measured as percentage deviations from that configuration mean.

The QQ-Plot [Figure~\ref{fig:qqplot}] compares the distribution of residuals against a theoretical normal distribution.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{images/qq_plot_residuals.pdf}
\caption{QQ-Plot of standardized residual percentages: sample quantiles (Y-axis) vs normal theoretical quantiles (X-axis).}
\label{fig:qqplot}
\end{figure}

The QQ-Plot shows a \textbf{non-linear behavior}, indicating that the residuals deviate from normality, particularly in the tails. This is expected given:

\begin{itemize}
    \item The wide range of configurations (from light load to saturation)
    \item The presence of two different distributions (Uniform vs Lognormal)
    \item The non-linear behavior near system saturation
\end{itemize}

The Shapiro-Wilk test confirms this departure from normality (p-value $\ll 0.05$).

\subsection{Homoskedasticity}

The residuals-vs-predicted plot [Figure~\ref{fig:residuals_pred}] checks whether residual variance is stable across operating conditions.
With perfect homoskedasticity, residual percentages should form a horizontal band around zero with similar vertical spread.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/residuals_vs_predicted.pdf}
\caption{Residual percentage (Y-axis) versus configuration mean waiting time in ms (X-axis, log scale).}
\label{fig:residuals_pred}
\end{figure}

In our case, a clear funnel shape is not sharp, but residual spread increases with configuration mean waiting time in stressed regimes. The rank trend between residual magnitude and this reference value is positive and significant (Spearman $\rho \approx 0.63$, $p \ll 0.001$), indicating \textbf{weak-to-moderate heteroskedasticity} concentrated in high-load/hotspot conditions.

Implications:
\begin{itemize}
    \item Point predictions are still useful (mean residual remains near zero).
    \item Uncertainty may grow only in the most stressed configurations.
    \item Main qualitative conclusions are unchanged.
\end{itemize}

\subsection{Residual Magnitude Analysis}

To quantify this effect, we examine the absolute residual percentage $|e|$ across all runs and by groups [Figure~\ref{fig:residual_mag}].

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/residual_magnitude.pdf}
\caption{Residual magnitude analysis: boxplots of $|$residual$|$ (\%) grouped by distribution and $p$.}
\label{fig:residual_mag}
\end{figure}

Most replications are very stable (median $|e| = 0.068\%$, 90th percentile $= 0.715\%$), while the largest deviations are concentrated in hotspot cases (up to about $19\%$).
Therefore, variability is mostly localized in stressed scenarios and does not change the core conclusions: the number of users is the dominant driver, while distribution mainly affects tails and stall risk.

\paragraph{Why residuals do not always increase with load}
In this analysis, residuals are computed \emph{within each fixed configuration} $(N,p,\text{distribution})$ as deviations from that configuration mean (across 5 replications).  
For this reason, residual magnitude is not required to be monotonic in $N$: it measures replication variability at the same operating point, not the global increase of waiting time with load.

\paragraph{Interpretation of Lognormal, $p=0.3$}
The largest relative residuals appear near the transition to saturation (around $N \approx 1500$), where runs can diverge in how quickly queues explode.  
After the system is fully stalled (larger $N$), all replications become consistently very large, so \emph{relative} residuals can decrease even if \emph{absolute} waiting times remain enormous (hundreds of seconds in ms scale).  
So the pattern is: peak variability at the instability threshold, then high but more uniform delay in deep-stall regimes.

\section{Conclusions}

This simulation study has successfully analyzed the performance of a distributed database system under various configurations. The main conclusions are:

\subsection{System Capacity Analysis}

Based on the simulation results, we analyze system capacity using \textbf{average waiting time} as the primary metric for detecting stall conditions:
\begin{itemize}
    \item \textbf{OK}: Waiting time $< 200$ms (acceptable response time)
    \item \textbf{DEGRADED}: Waiting time $200$--$1000$ms (noticeable delays)
    \item \textbf{STALLED}: Waiting time $> 1000$ms (system overloaded)
\end{itemize}

\subsubsection{Detailed Throughput Analysis}

Figure~\ref{fig:capacity_heatmap} summarizes all 72 configurations as a waiting-time heatmap. The overall throughput grows with load, while waiting time increases sharply under hotspot conditions. The full detailed table is reported in Appendix~\ref{chap:appendix_table}.

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{images/capacity_heatmap_waittime.pdf}
\caption{Capacity heatmap for Uniform and Lognormal traffic. Colors represent mean waiting time (log scale). Numbers inside cells are mean waiting time in ms (compact format, e.g., 15k = 15,000ms).}
\label{fig:capacity_heatmap}
\end{figure}

\subsubsection{Capacity Guidelines}

\begin{itemize}
    \item \textbf{Uniform Distribution}:
    \begin{itemize}
        \item $p=0.3$: Max \textbf{2,500 users} (W$<$200ms), stalls at N$>$4,000
        \item $p=0.5$: Max \textbf{3,000 users} (W$<$200ms), degraded but stable up to 5,000
        \item $p=0.8$: Max \textbf{5,000+ users} (W=154ms at N=5,000) -- \textbf{best configuration}
    \end{itemize}
    
    \item \textbf{Lognormal Distribution (hotspots)}:
    \begin{itemize}
        \item $p=0.3$: Max \textbf{1,200 users} only -- hotspots cause early saturation
        \item $p=0.5$: Max \textbf{1,200 users} for W$<$200ms (up to 1,600 for W$<$1s)
        \item $p=0.8$: Max \textbf{2,500 users} for W$<$200ms (up to 3,500 for W$<$1s)
    \end{itemize}
\end{itemize}

\begin{tcolorbox}[colback=yellow!10, title=Critical Finding: Hotspot Impact]
The \textbf{Lognormal distribution} (simulating hotspot access patterns) reduces system capacity by about \textbf{50--60\%} compared to Uniform distribution:
\begin{itemize}
    \item At $p=0.3$: 1,200 vs 2,500 users (52\% reduction)
    \item At $p=0.5$: 1,200 vs 3,000 users (60\% reduction)
    \item At $p=0.8$: 2,500 vs 5,000+ users (50\% reduction)
\end{itemize}
This highlights the critical importance of \textbf{load balancing} in production systems.
\end{tcolorbox}

\begin{tcolorbox}[title=Key Conclusions]
\begin{enumerate}
    \item \textbf{Model Behavior}: Across all runs, throughput scales almost linearly with offered load, while waiting time captures saturation and hotspot effects.
    
    \item \textbf{Capacity Limits} (for $M=20$, $\lambda=0.05$, $S=0.1$s):
    \begin{itemize}
        \item \textbf{Best case} (Uniform, $p=0.8$): 5,000+ concurrent users
        \item \textbf{Typical} (Uniform, $p=0.5$): 3,000 concurrent users
        \item \textbf{Worst case} (Lognormal, $p=0.3$): 1,200 concurrent users
    \end{itemize}
    
    \item \textbf{Stall Detection}: System enters stall when:
    \begin{itemize}
        \item Average waiting time exceeds 1 second
        \item Queueing delay grows explosively in hotspot configurations
    \end{itemize}
    
    \item \textbf{Read/Write Impact}: Higher read probability increases usable capacity, with the strongest gains under hotspot traffic.
\end{enumerate}
\end{tcolorbox}
