\chapter{Data Analysis and Conclusions}

This chapter presents a statistical analysis of the simulation results, including warm-up period determination, factorial design analysis, normality testing, and residual analysis to validate the model assumptions.

\section{Warm-Up Period Analysis}

Before collecting steady-state statistics, it is essential to identify and discard the \textbf{transient phase} (warm-up period) during which the system has not yet reached equilibrium.

In this project, the warm-up period was estimated by \textbf{visual inspection} of the simulation trend.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/warmup_analysis.png}
\caption{Warm-up behavior observed in the simulation output.}
\label{fig:warmup_analysis}
\end{figure}

From Figure~\ref{fig:warmup_analysis}, it is clear that after the initial transient the curve stabilizes, so a \textbf{500 s warm-up} is considered sufficient.

\begin{tcolorbox}[title=Warm-Up Configuration]
The OMNeT++ configuration includes:
\begin{verbatim}
warmup-period = 500s
\end{verbatim}
\end{tcolorbox}

\section{Experimental Design}

The simulation study follows a \textbf{factorial design} with the following factors:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Factor} & \textbf{Symbol} & \textbf{Levels} \\
\hline
Number of users & $N$ & 100, 500, 1000, 1200, 1500, 1600, 2000, 2500, 3000, 3500, 4000, 5000 \\
Read probability & $p$ & 0.3, 0.5, 0.8 \\
Number of tables & $M$ & 20 \\
Distribution type & - & Uniform, Lognormal \\
\hline
\end{tabular}
\caption{Experimental factors and their levels}
\label{tab:factors}
\end{table}

Each configuration was replicated 5 times with different random seeds, resulting in a total of \textbf{360 simulation runs}.

\section{Factor Impact Analysis}

The pie chart [Figure~\ref{fig:factor_pie}] shows how much each factor contributes to \textbf{throughput} variability.
The key message is that the largest slice is \textbf{Number of Users ($N$)} (about \textbf{72.5\%}), so system load is the main driver of throughput.
This does not contradict the capacity results: distribution has limited impact on aggregate throughput, but a strong impact on waiting-time tails and stall threshold.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/factor_effects_pie.pdf}
\caption{The pie chart illustrates the impact of each factor on the throughput of the system.}
\label{fig:factor_pie}
\end{figure}

\subsection{Key Findings}

The analysis reveals that:

\begin{itemize}
    \item \textbf{Number of Users ($N$)}: This is the most significant factor by far. In practice, throughput behavior is mostly determined by how many users are active.
    
    \item \textbf{Read Probability ($p$)}: Secondary effect. It influences performance, but much less than $N$.
    
    \item \textbf{Distribution effect}: The distribution has a moderate impact on global throughput, but a strong impact on delay and stability. Lognormal hotspots can trigger queue explosion and early stall under high load.
\end{itemize}


\section{Model Validation}

\subsection{Testing Normality Hypothesis}

The QQ-Plot [Figure~\ref{fig:qqplot}] compares the distribution of residuals against a theoretical normal distribution.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{images/qq_plot_residuals.pdf}
\caption{QQ-Plot: On Y-Axis the sample quantiles and X-Axis the theoretical quantiles of a normal distribution.}
\label{fig:qqplot}
\end{figure}

The QQ-Plot shows a \textbf{non-linear behavior}, indicating that the residuals deviate from normality, particularly in the tails. This is expected given:

\begin{itemize}
    \item The wide range of configurations (from light load to saturation)
    \item The presence of two different distributions (Uniform vs Lognormal)
    \item The non-linear behavior near system saturation
\end{itemize}

The Shapiro-Wilk test confirms this departure from normality (p-value $< 0.05$).

\subsection{Homoskedasticity}

The residuals-vs-predicted plot [Figure~\ref{fig:residuals_pred}] is used to check whether error variance is constant across operating conditions.
With perfect homoskedasticity, residuals should form a horizontal band around zero with similar vertical spread for all predicted values.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/residuals_vs_predicted.pdf}
\caption{The residuals are plotted along the Y-axis, while the predicted responses are represented on the X-axis.}
\label{fig:residuals_pred}
\end{figure}

In our case, a clear funnel shape is not evident. Residuals remain centered around zero and the vertical spread is fairly stable across predicted values, with only slight widening at the highest loads. Overall, this is consistent with \textbf{approximately homoskedastic} behavior (at most weak heteroskedasticity in extreme scenarios).

Implications:
\begin{itemize}
    \item Point predictions are still useful (no strong systematic bias in the mean residual).
    \item Uncertainty may grow only in the most stressed configurations.
    \item Main qualitative conclusions are unchanged.
\end{itemize}

\subsection{Residual Magnitude Analysis}

To quantify this effect, we also examine the relative error magnitude $|$residual$| / |$predicted$|$ [Figure~\ref{fig:residual_mag}].

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/residual_magnitude.pdf}
\caption{Each residual varies by an order of magnitude or more below the predicted response. On Y-Axis the ratio $|$residual$|$ / $|$predicted$|$ and X-Axis the observation number.}
\label{fig:residual_mag}
\end{figure}

Most observations remain \textbf{below 10\%}, while larger ratios are concentrated in stressed/hotspot scenarios.
Therefore, heteroskedasticity is present but does not invalidate the core conclusions of this study: the number of users is the dominant driver, and distribution mostly affects tail behavior (localized queue explosions).

\section{Conclusions}

This simulation study has successfully analyzed the performance of a distributed database system under various configurations. The main conclusions are:

\subsection{System Capacity Analysis}

Based on the simulation results, we analyze system capacity using \textbf{average waiting time} as the primary metric for detecting stall conditions:
\begin{itemize}
    \item \textbf{OK}: Waiting time $< 200$ms (acceptable response time)
    \item \textbf{DEGRADED}: Waiting time $200$--$1000$ms (noticeable delays)
    \item \textbf{STALLED}: Waiting time $> 1000$ms (system overloaded)
\end{itemize}

\subsubsection{Detailed Throughput Analysis}

Figure~\ref{fig:capacity_heatmap} summarizes all 72 configurations as a waiting-time heatmap. The overall throughput grows with load, while waiting time increases sharply under hotspot conditions. The full detailed table is reported in Appendix~\ref{chap:appendix_table}.

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{images/capacity_heatmap_waittime.pdf}
\caption{Capacity heatmap for Uniform and Lognormal traffic. Colors represent mean waiting time (log scale). Numbers inside cells are mean waiting time in ms (compact format, e.g., 15k = 15,000ms).}
\label{fig:capacity_heatmap}
\end{figure}

\subsubsection{Capacity Guidelines}

\begin{itemize}
    \item \textbf{Uniform Distribution}:
    \begin{itemize}
        \item $p=0.3$: Max \textbf{2,500 users} (W$<$200ms), stalls at N$>$4,000
        \item $p=0.5$: Max \textbf{3,000 users} (W$<$200ms), degraded but stable up to 5,000
        \item $p=0.8$: Max \textbf{5,000+ users} (W=154ms at N=5,000) -- \textbf{best configuration}
    \end{itemize}
    
    \item \textbf{Lognormal Distribution (hotspots)}:
    \begin{itemize}
        \item $p=0.3$: Max \textbf{1,200 users} only -- hotspots cause early saturation
        \item $p=0.5$: Max \textbf{1,200 users} for W$<$200ms (up to 1,600 for W$<$1s)
        \item $p=0.8$: Max \textbf{2,500 users} for W$<$200ms (up to 3,500 for W$<$1s)
    \end{itemize}
\end{itemize}

\begin{tcolorbox}[colback=yellow!10, title=Critical Finding: Hotspot Impact]
The \textbf{Lognormal distribution} (simulating hotspot access patterns) reduces system capacity by about \textbf{50--60\%} compared to Uniform distribution:
\begin{itemize}
    \item At $p=0.3$: 1,200 vs 2,500 users (52\% reduction)
    \item At $p=0.5$: 1,200 vs 3,000 users (60\% reduction)
    \item At $p=0.8$: 2,500 vs 5,000+ users (50\% reduction)
\end{itemize}
This highlights the critical importance of \textbf{load balancing} in production systems.
\end{tcolorbox}

\begin{tcolorbox}[title=Key Conclusions]
\begin{enumerate}
    \item \textbf{Model Behavior}: Across all runs, throughput scales almost linearly with offered load, while waiting time captures saturation and hotspot effects.
    
    \item \textbf{Capacity Limits} (for $M=20$, $\lambda=0.05$, $S=0.1$s):
    \begin{itemize}
        \item \textbf{Best case} (Uniform, $p=0.8$): 5,000+ concurrent users
        \item \textbf{Typical} (Uniform, $p=0.5$): 3,000 concurrent users
        \item \textbf{Worst case} (Lognormal, $p=0.3$): 1,200 concurrent users
    \end{itemize}
    
    \item \textbf{Stall Detection}: System enters stall when:
    \begin{itemize}
        \item Average waiting time exceeds 1 second
        \item Queueing delay grows explosively in hotspot configurations
    \end{itemize}
    
    \item \textbf{Read/Write Impact}: Higher read probability increases usable capacity, with the strongest gains under hotspot traffic.
\end{enumerate}
\end{tcolorbox}
