[General]
network = progetto.DatabaseNetwork
sim-time-limit = 10000s
cpu-time-limit = 3600s

# --- Ripetizioni multiple con seed diversi ---
repeat = 5
seed-set = ${repetition}

# --- Output e Risultati ---
output-scalar-file = ${resultdir}/${configname}-${iterationvars}-${repetition}.sca
result-dir = results
**.vector-recording = false
**.scalar-recording = true
**.result-recording-modes = -vector

# --- Parametri Globali ---
# M = numero tabelle
# N = numero utenti
# T = tempo medio tra accessi (esponenziale con rate lambda = 1/T)
# S = durata fissa operazione
# p = probabilità di read

# Lambda = 1/T dove T è il tempo medio tra accessi
# Con lambda=0.05 -> T medio = 20 secondi
*.user[*].lambda = 0.05

# Durata operazione database (100ms, tipico per query semplici)
*.user[*].serviceTime = 0.1s

# --- Configurazione Uniform con parametri multipli ---
[Config Uniform]
description = "Uniform distribution - varia M, N e p"
*.user[*].tableDistribution = "uniform"

# Numero tabelle: 10, 20
*.numTables = ${20}

# Numero di utenti: da 100 a 5000 per testare saturazione
*.numUsers = ${N=100, 500, 1000, 1200, 1500, 1600, 2000, 2500, 3000, 3500, 4000, 5000}

# Probabilità di read: 0.3, 0.5, 0.8Summarizing conversation history...
*.user[*].readProbability = ${p=0.3, 0.5, 0.8}

# Questo genererà automaticamente 18 run (6 valori di N × 3 valori di p)


[Config Lognormal]
description = "Lognormal distribution (hotspots) - varia M, N e p"
*.user[*].tableDistribution = "lognormal"
*.user[*].lognormalM = 1.5
*.user[*].lognormalS = 1.0

# Numero tabelle: 10, 20
*.numTables = ${20}

# Numero di utenti: da 100 a 5000 per testare saturazione
*.numUsers = ${N=100, 500, 1000, 1200, 1500, 1600, 2000, 2500, 3000, 3500, 4000, 5000}

# Probabilità di read: 0.3, 0.5, 0.8
*.user[*].readProbability = ${p=0.3, 0.5, 0.8}

# Questo genererà automaticamente 18 run (6 valori di N × 3 valori di p)